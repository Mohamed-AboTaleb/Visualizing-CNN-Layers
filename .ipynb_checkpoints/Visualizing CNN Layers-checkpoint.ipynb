{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the following pytorch excercise by Udacity: (Lesson 5.9 of [this](https://www.udacity.com/course/deep-learning-pytorch--ud188) course)\n",
    "\n",
    "    Part 3 - Training Neural Networks (Exercises)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transform \n",
    "# for this we need to convert the input to torch-tensors using transforms.ToTensor()\n",
    "# we will be working with MNIST dataset which are gray scale images\n",
    "# V.Imp: Since the MNIST images have a  single channel, therefore one might be tempted to \n",
    "# call transforms.Normalize((0.5), (0.5)) instead of transforms.Normalize((0.5,), (0.5,))\n",
    "# The difference is the presence of a comma after 0.5 in the second call\n",
    "# then this will throw an error \"too many indices for tensor of dimension 0\"\n",
    "# this is because transforms.Normalize wants a tuple to be passed as its mean and standard\n",
    "# however torch interprets (0.5) as a number and NOT a tuple. So we need to pass (0.5, )\n",
    "# This is well explained by Berriel in the following stackoverflow post\n",
    "# https://stackoverflow.com/questions/56745486/pytorch-dataloader-indexerror-too-many-indices-for-tensor-of-dimension-0\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5, ), (0.5, ))]) \n",
    "\n",
    "# import the MNIST dataset\n",
    "train = datasets.MNIST('~/.pytorch/MNIST_data/', download = True, train = True, transform = transform)\n",
    "# Dataloader\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each image has shape: (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checkout a random image in the training set\n",
    "image, label = next(iter(trainloader))\n",
    "img=torch.squeeze(image[0],0).numpy()\n",
    "print('Each image has shape: {}'.format(img.shape))\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.title(\"The number is {}\".format(label[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a simple CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architechture:\n",
    "# 1st Conv. layer with 32 filters; Each having a kernel of size 3, stride = 1, and default padding and dilation\n",
    "# this will reduce the shape of the input tensor from (1,28,28) to (32,26,26)\n",
    "# then apply Relu activation\n",
    "# 2nd Conv. layer with 16 filters, each having a kernel size 5, stride = 1, default padding and dilation\n",
    "# the output of this layer will have shape (16, 22, 22)\n",
    "# again apply ReLU to this output\n",
    "# Now flatten the output to pass to a linear NN for classification\n",
    "# The output of this layer will be a 1d array of 16*22*22 = 7744 elements\n",
    "# We now add a linear layer of 32 nuerons\n",
    "\n",
    "# the model has to be a class that inherits from nn.Module\n",
    "# note that we can define relu once and use it repeatedly. Also see the following discussion\n",
    "# https://discuss.pytorch.org/t/using-same-dropout-object-for-multiple-drop-out-layers/39027\n",
    "class my_CNN(nn.Module):\n",
    "    # class constructor (i.e. initializer)\n",
    "    def __init__(self):\n",
    "        # initialize the base class\n",
    "        super().__init__()\n",
    "        \n",
    "        self.l1 = nn.Conv2d(1, 32, 3 )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.l2 = nn.Conv2d(32, 16, 5 )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.l3 = nn.Linear(7744, 32, bias = True)\n",
    "        \n",
    "        self.l4 = nn.Linear(32, 10, bias = True)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.l1(x)\n",
    "        act1 = self.relu(out1)\n",
    "        out2 = self.l2(act1)\n",
    "        act2 = self.relu(out2)\n",
    "        flat = self.flatten(act2)\n",
    "        out3 = self.l3(flat)\n",
    "        act3 = self.relu(out3)\n",
    "        out4 = self.l4(act3)\n",
    "        \n",
    "        output = self.softmax(out4)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_CNN(\n",
       "  (l1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (l2): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (flatten): Flatten()\n",
       "  (l3): Linear(in_features=7744, out_features=32, bias=True)\n",
       "  (l4): Linear(in_features=32, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = my_CNN()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
